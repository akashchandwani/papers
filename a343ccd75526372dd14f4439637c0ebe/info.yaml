abstract: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks
  by pre-training on a large corpus of text followed by fine-tuning on a specific
  task. While typically task-agnostic in architecture, this method still requires
  task-specific fine-tuning datasets of thousands or tens of thousands of examples.
  By contrast, humans can generally perform a new language task from only a few examples
  or from simple instructions - something which current NLP systems still largely
  struggle to do. Here we show that scaling up language models greatly improves task-agnostic,
  few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art
  fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language
  model with 175 billion parameters, 10x more than any previous non-sparse language
  model, and test its performance in the few-shot setting. For all tasks, GPT-3 is
  applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations
  specified purely via text interaction with the model. GPT-3 achieves strong performance
  on many NLP datasets, including translation, question-answering, and cloze tasks,
  as well as several tasks that require on-the-fly reasoning or domain adaptation,
  such as unscrambling words, using a novel word in a sentence, or performing 3-digit
  arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot
  learning still struggles, as well as some datasets where GPT-3 faces methodological
  issues related to training on large web corpora. Finally, we find that GPT-3 can
  generate samples of news articles which human evaluators have difficulty distinguishing
  from articles written by humans. We discuss broader societal impacts of this finding
  and of GPT-3 in general.
author: Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and
  Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and
  Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel
  and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and
  Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and
  Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin
  and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and
  Sutskever, Ilya and Amodei, Dario
author_list:
- affiliation: []
  family: Brown
  given: Tom B.
- affiliation: []
  family: Mann
  given: Benjamin
- affiliation: []
  family: Ryder
  given: Nick
- affiliation: []
  family: Subbiah
  given: Melanie
- affiliation: []
  family: Kaplan
  given: Jared
- affiliation: []
  family: Dhariwal
  given: Prafulla
- affiliation: []
  family: Neelakantan
  given: Arvind
- affiliation: []
  family: Shyam
  given: Pranav
- affiliation: []
  family: Sastry
  given: Girish
- affiliation: []
  family: Askell
  given: Amanda
- affiliation: []
  family: Agarwal
  given: Sandhini
- affiliation: []
  family: Herbert-Voss
  given: Ariel
- affiliation: []
  family: Krueger
  given: Gretchen
- affiliation: []
  family: Henighan
  given: Tom
- affiliation: []
  family: Child
  given: Rewon
- affiliation: []
  family: Ramesh
  given: Aditya
- affiliation: []
  family: Ziegler
  given: Daniel M.
- affiliation: []
  family: Wu
  given: Jeffrey
- affiliation: []
  family: Winter
  given: Clemens
- affiliation: []
  family: Hesse
  given: Christopher
- affiliation: []
  family: Chen
  given: Mark
- affiliation: []
  family: Sigler
  given: Eric
- affiliation: []
  family: Litwin
  given: Mateusz
- affiliation: []
  family: Gray
  given: Scott
- affiliation: []
  family: Chess
  given: Benjamin
- affiliation: []
  family: Clark
  given: Jack
- affiliation: []
  family: Berner
  given: Christopher
- affiliation: []
  family: McCandlish
  given: Sam
- affiliation: []
  family: Radford
  given: Alec
- affiliation: []
  family: Sutskever
  given: Ilya
- affiliation: []
  family: Amodei
  given: Dario
doi: 10.48550/arXiv.2005.14165
files:
- 2005.14165v4.pdf
online_date: 2020/07/22
papis_id: a343ccd75526372dd14f4439637c0ebe
pdf_url: http://arxiv.org/pdf/2005.14165
ref: Language_Models_Brown
time-added: 2025-08-21-01:51:20
title: Language Models are Few-Shot Learners
type: website
url: https://arxiv.org/abs/2005.14165v4
